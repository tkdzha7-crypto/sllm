import json
import os
import pickle
import sys
from datetime import datetime

import numpy as np
import pandas as pd

from flows.common.db_utils import ensure_partition_exists, fast_bulk_insert, get_engine
from src.models.recommendation.category_mapper import (
    get_category_name,
    load_category_mapping,
)
from src.models.recommendation.mapping_constants import ì—…íƒœ_TO_ë¶„ë¥˜, ì¢…ëª©_TO_ë¶„ë¥˜


class UserRecommender:
    def __init__(
        self,
        inference_mode="current_user",
        model_path="src/models/simple_clustering_model_pca.pkl",
        mapping_dir="src/models",
    ):
        self.inference_mode = inference_mode
        self.model_path = model_path
        self.mapping_dir = mapping_dir
        self.ì—…íƒœ_to_ë¶„ë¥˜ = ì—…íƒœ_TO_ë¶„ë¥˜
        self.ì¢…ëª©_to_ë¶„ë¥˜ = ì¢…ëª©_TO_ë¶„ë¥˜

        # Load category mapping once at initialization
        self.category_mapping = load_category_mapping()

        # Model artifacts
        self.kmeans = None
        self.scaler = None
        self.pca = None
        self.le_sido = None
        self.le_sigungu = None
        self.le_ëŒ€ë¶„ë¥˜ = None
        self.le_ì¤‘ë¶„ë¥˜ = None
        self.le_ì†Œë¶„ë¥˜ = None
        self.le_ì„¸ë¶„ë¥˜ = None
        self.le_ì—…íƒœ = None
        self.cluster_recommendations = None
        self.clustering_df = None

        self.feature_cols = [
            "ìœ„ë„",
            "ê²½ë„",
            "ì‹œë„_encoded",
            "ì‹œêµ°êµ¬_encoded",
            "ë¶„ë¥˜_PCA1",
            "ë¶„ë¥˜_PCA2",
            "ì—…íƒœ_encoded",
            "í‰ê· _ë©´ì _category",
        ]

        self.load_model_artifacts()

    def load_model_artifacts(self):
        """Load trained PCA model, encoders, PCA transformer, and cluster recommendations"""
        try:
            with open(self.model_path, "rb") as f:
                artifacts = pickle.load(f)

            self.kmeans = artifacts["kmeans"]
            self.scaler = artifacts["scaler"]
            self.pca = artifacts["pca"]
            self.le_sido = artifacts["le_sido"]
            self.le_sigungu = artifacts["le_sigungu"]
            self.le_ëŒ€ë¶„ë¥˜ = artifacts["le_ëŒ€ë¶„ë¥˜"]
            self.le_ì¤‘ë¶„ë¥˜ = artifacts["le_ì¤‘ë¶„ë¥˜"]
            self.le_ì†Œë¶„ë¥˜ = artifacts["le_ì†Œë¶„ë¥˜"]
            self.le_ì„¸ë¶„ë¥˜ = artifacts["le_ì„¸ë¶„ë¥˜"]
            self.le_ì—…íƒœ = artifacts["le_ì—…íƒœ"]
            self.cluster_recommendations = artifacts["cluster_recommendations"]
            self.clustering_df = artifacts["clustering_df"]

            print("âœ… PCA model loaded successfully")
            print("   - Features: 8 (with PCA dimensionality reduction)")
            print(f"   - Clusters: {self.kmeans.n_clusters}")

        except FileNotFoundError:
            print(
                "âŒ Error: PCA model file not found. Please train the model first using the notebook."
            )
            sys.exit(1)

    def map_code_to_name(self, code):
        """Map industry code to name using cached mapping."""
        return get_category_name(code, self.category_mapping)

    def update_cluster_info(self, snapshot_month: str):
        # Ensure the user_monthly_features partition exists
        snapshot_date = datetime.strptime(snapshot_month, "%Y_%m").replace(day=1)
        ensure_partition_exists(
            target_date=snapshot_date,
            schema="source",
            table_name="user_monthly_features",
        )

        query = """
        SELECT
            ur.CCOD,
            ur.user_cluster,
            umf.user_information,
            umf.contract_info,
            umf.purchase_logs
        FROM analytics.user_recommendation_{snapshot_month} ur
            LEFT JOIN source.user_monthly_features_{snapshot_month} umf
            on ur.CCOD = umf.CCOD
        """
        query = query.format(snapshot_month=snapshot_month)
        engine = get_engine()
        df = pd.read_sql(query, engine)

        # If no data, skip cluster profile update
        if df.empty:
            print(
                f"âš ï¸ No data found for snapshot month {snapshot_month}. Skipping cluster profile update."
            )
            return

        # Pre-process JSON strings into objects
        def safe_json_loads(x):
            if isinstance(x, str):
                try:
                    return json.loads(x)
                except json.JSONDecodeError:
                    return None
            return x

        df["user_information"] = df["user_information"].apply(safe_json_loads)
        df["contract_info"] = (
            df["contract_info"]
            .apply(safe_json_loads)
            .apply(lambda x: x if isinstance(x, list) else [])
        )
        df["purchase_logs"] = (
            df["purchase_logs"]
            .apply(safe_json_loads)
            .apply(lambda x: x if isinstance(x, list) else [])
        )

        cluster_summary = (
            df.groupby("user_cluster")
            .agg(
                cluster_size=("ccod", "nunique"),
                avg_contracts_num=(
                    "contract_info",
                    lambda x: np.mean([len(c) if c else 0 for c in x]),
                ),
                avg_purchase_num=(
                    "purchase_logs",
                    lambda x: np.mean([len(p) if p else 0 for p in x]),
                ),
                top_contracts=(
                    "contract_info",
                    lambda x: pd.Series(
                        [
                            item.get("ê³„ì•½ëŒ€ìƒ")
                            for sublist in x.dropna()
                            for item in sublist
                            if item and "ê³„ì•½ëŒ€ìƒ" in item
                        ]
                    )
                    .value_counts()
                    .head(5)
                    .index.tolist(),
                ),
                top_purchases=(
                    "purchase_logs",
                    lambda x: pd.Series(
                        [
                            item.get("service_name")
                            for sublist in x.dropna()
                            for item in sublist
                            if item and "service_name" in item
                        ]
                    )
                    .value_counts()
                    .head(5)
                    .index.tolist(),
                ),
                top_business_type=(
                    "user_information",
                    lambda x: pd.Series(
                        [
                            info.get("ì—…íƒœ")
                            for info in x.dropna()
                            if info and "ì—…íƒœ" in info
                        ]
                    )
                    .value_counts()
                    .head(5)
                    .index.tolist(),
                ),
                top_first_contract_code=(
                    "contract_info",
                    lambda x: pd.Series(
                        [
                            c[0].get("ê³„ì•½ëŒ€ìƒ")
                            for c in x.dropna()
                            if c and len(c) > 0 and c[0] and "ê³„ì•½ëŒ€ìƒ" in c[0]
                        ]
                    )
                    .value_counts()
                    .head(5)
                    .index.tolist(),
                ),
                contracts_distribution=(
                    "contract_info",
                    lambda x: pd.Series([len(c) for c in x.dropna() if c])
                    .value_counts()
                    .to_dict(),
                ),
                purchase_distribution=(
                    "purchase_logs",
                    lambda x: pd.Series([len(p) for p in x.dropna() if p])
                    .value_counts()
                    .to_dict(),
                ),
            )
            .reset_index()
        )
        print(cluster_summary.head())

        snapshot_month_date = datetime.now().strftime("%Y-%m-01")
        cluster_profile_df = pd.DataFrame()
        cluster_profile_df["snapshot_month"] = [snapshot_month_date] * len(
            cluster_summary
        )
        cluster_profile_df["cluster_id"] = cluster_summary["user_cluster"]
        cluster_profile_df["cluster_size"] = cluster_summary["cluster_size"]
        cluster_profile_df["avg_contracts_num"] = cluster_summary["avg_contracts_num"]
        cluster_profile_df["avg_purchase_num"] = cluster_summary["avg_purchase_num"]
        cluster_profile_df["top_contracts"] = cluster_summary["top_contracts"].apply(
            lambda x: json.dumps(x, ensure_ascii=False)
        )
        cluster_profile_df["top_purchases"] = cluster_summary["top_purchases"].apply(
            lambda x: json.dumps(x, ensure_ascii=False)
        )
        cluster_profile_df["top_business_type"] = cluster_summary[
            "top_business_type"
        ].apply(lambda x: json.dumps(x, ensure_ascii=False))
        cluster_profile_df["top_first_contract_code"] = cluster_summary[
            "top_first_contract_code"
        ].apply(lambda x: json.dumps(x, ensure_ascii=False))
        cluster_profile_df["contracts_distribution"] = cluster_summary[
            "contracts_distribution"
        ].apply(lambda x: json.dumps(x, ensure_ascii=False))
        cluster_profile_df["purchase_distribution"] = cluster_summary[
            "purchase_distribution"
        ].apply(lambda x: json.dumps(x, ensure_ascii=False))
        ensure_partition_exists(
            schema="analytics",
            table_name="cluster_profile",
            target_date=datetime.strptime(snapshot_month_date, "%Y-%m-%d"),
        )
        fast_bulk_insert(
            cluster_profile_df, table_name="cluster_profile", schema="analytics"
        )

    @staticmethod
    def extract_sido(address):
        """Extract ì‹œë„ from address"""
        if pd.isna(address):
            return None
        address = str(address).strip()

        sido_list = [
            "ì„œìš¸íŠ¹ë³„ì‹œ",
            "ì„œìš¸",
            "ë¶€ì‚°ê´‘ì—­ì‹œ",
            "ë¶€ì‚°",
            "ëŒ€êµ¬ê´‘ì—­ì‹œ",
            "ëŒ€êµ¬",
            "ì¸ì²œê´‘ì—­ì‹œ",
            "ì¸ì²œ",
            "ê´‘ì£¼ê´‘ì—­ì‹œ",
            "ê´‘ì£¼",
            "ëŒ€ì „ê´‘ì—­ì‹œ",
            "ëŒ€ì „",
            "ìš¸ì‚°ê´‘ì—­ì‹œ",
            "ìš¸ì‚°",
            "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
            "ì„¸ì¢…",
            "ê²½ê¸°ë„",
            "ê²½ê¸°",
            "ê°•ì›ë„",
            "ê°•ì›íŠ¹ë³„ìì¹˜ë„",
            "ê°•ì›",
            "ì¶©ì²­ë¶ë„",
            "ì¶©ë¶",
            "ì¶©ì²­ë‚¨ë„",
            "ì¶©ë‚¨",
            "ì „ë¼ë¶ë„",
            "ì „ë¶",
            "ì „ë¶íŠ¹ë³„ìì¹˜ë„",
            "ì „ë¼ë‚¨ë„",
            "ì „ë‚¨",
            "ê²½ìƒë¶ë„",
            "ê²½ë¶",
            "ê²½ìƒë‚¨ë„",
            "ê²½ë‚¨",
            "ì œì£¼íŠ¹ë³„ìì¹˜ë„",
            "ì œì£¼",
        ]

        for sido in sido_list:
            if address.startswith(sido):
                if sido in ["ì„œìš¸", "ì„œìš¸íŠ¹ë³„ì‹œ"]:
                    return "ì„œìš¸íŠ¹ë³„ì‹œ"
                elif sido in ["ë¶€ì‚°", "ë¶€ì‚°ê´‘ì—­ì‹œ"]:
                    return "ë¶€ì‚°ê´‘ì—­ì‹œ"
                elif sido in ["ëŒ€êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ"]:
                    return "ëŒ€êµ¬ê´‘ì—­ì‹œ"
                elif sido in ["ì¸ì²œ", "ì¸ì²œê´‘ì—­ì‹œ"]:
                    return "ì¸ì²œê´‘ì—­ì‹œ"
                elif sido in ["ê´‘ì£¼", "ê´‘ì£¼ê´‘ì—­ì‹œ"]:
                    return "ê´‘ì£¼ê´‘ì—­ì‹œ"
                elif sido in ["ëŒ€ì „", "ëŒ€ì „ê´‘ì—­ì‹œ"]:
                    return "ëŒ€ì „ê´‘ì—­ì‹œ"
                elif sido in ["ìš¸ì‚°", "ìš¸ì‚°ê´‘ì—­ì‹œ"]:
                    return "ìš¸ì‚°ê´‘ì—­ì‹œ"
                elif sido in ["ì„¸ì¢…", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"]:
                    return "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"
                elif sido in ["ê²½ê¸°", "ê²½ê¸°ë„"]:
                    return "ê²½ê¸°ë„"
                elif sido in ["ê°•ì›", "ê°•ì›ë„", "ê°•ì›íŠ¹ë³„ìì¹˜ë„"]:
                    return "ê°•ì›íŠ¹ë³„ìì¹˜ë„"
                elif sido in ["ì¶©ë¶", "ì¶©ì²­ë¶ë„"]:
                    return "ì¶©ì²­ë¶ë„"
                elif sido in ["ì¶©ë‚¨", "ì¶©ì²­ë‚¨ë„"]:
                    return "ì¶©ì²­ë‚¨ë„"
                elif sido in ["ì „ë¶", "ì „ë¼ë¶ë„", "ì „ë¶íŠ¹ë³„ìì¹˜ë„"]:
                    return "ì „ë¶íŠ¹ë³„ìì¹˜ë„"
                elif sido in ["ì „ë‚¨", "ì „ë¼ë‚¨ë„"]:
                    return "ì „ë¼ë‚¨ë„"
                elif sido in ["ê²½ë¶", "ê²½ìƒë¶ë„"]:
                    return "ê²½ìƒë¶ë„"
                elif sido in ["ê²½ë‚¨", "ê²½ìƒë‚¨ë„"]:
                    return "ê²½ìƒë‚¨ë„"
                elif sido in ["ì œì£¼", "ì œì£¼íŠ¹ë³„ìì¹˜ë„"]:
                    return "ì œì£¼íŠ¹ë³„ìì¹˜ë„"
                return sido
        return None

    @staticmethod
    def extract_sigungu(address):
        """Extract ì‹œêµ°êµ¬ from address"""
        if pd.isna(address):
            return None
        address = str(address).strip()

        sido = UserRecommender.extract_sido(address)
        if sido:
            address = address.replace(sido, "").strip()

        parts = address.split()
        if len(parts) > 0:
            sigungu = parts[0]
            if "(" in sigungu:
                sigungu = sigungu.split("(")[0].strip()
            return sigungu
        return None

    @staticmethod
    def map_ì—…ì¢…ëª…_to_ì—…íƒœ(ì—…ì¢…ëª…_value):
        """ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ ëœ ì—…ì¢…ëª… â†’ ì—…íƒœ ë§¤í•‘"""
        if pd.isna(ì—…ì¢…ëª…_value):
            return None

        ì—…ì¢…ëª… = str(ì—…ì¢…ëª…_value).strip()

        if any(
            kw in ì—…ì¢…ëª…
            for kw in [
                "ìŒì‹ì ",
                "í•œì‹",
                "ì¤‘ì‹",
                "ì¼ì‹",
                "ì–‘ì‹",
                "ë¶„ì‹",
                "ì¹˜í‚¨",
                "ì¹´í˜",
                "ì»¤í”¼",
                "ì£¼ì ",
                "ì‹ë‹¹",
                "í”¼ì",
                "í–„ë²„ê±°",
                "ìƒŒë“œìœ„ì¹˜",
                "ë² ì´ì»¤ë¦¬",
            ]
        ):
            return "ìŒì‹ì ì—…"

        if any(
            kw in ì—…ì¢…ëª…
            for kw in [
                "ì†Œë§¤",
                "ìŠˆí¼ë§ˆì¼“",
                "í¸ì˜ì ",
                "ë§ˆíŠ¸",
                "ë„ë§¤",
                "ì¢…í•© ì†Œë§¤",
                "ë¹µë¥˜",
                "ê³¼ìë¥˜",
                "ìœ¡ë¥˜",
                "ë¬¸êµ¬",
                "í™”ì´ˆ",
                "ì‹ë¬¼",
                "ì£¼ë°©ìš©í’ˆ",
                "ê°€ì „",
                "ì˜ë³µ",
                "ìƒí’ˆ",
            ]
        ):
            return "ë„ì†Œë§¤"

        if any(kw in ì—…ì¢…ëª… for kw in ["ì œì¡°", "ìƒì‚°", "ê³µì¥", "ì œì‘", "ì¸ì‡„", "ì œí’ˆ"]):
            return "ì œì¡°ì—…"

        if any(kw in ì—…ì¢…ëª… for kw in ["êµìœ¡", "í•™ì›", "í›ˆë ¨"]):
            return "êµìœ¡ì—…"

        if any(
            kw in ì—…ì¢…ëª…
            for kw in [
                "ì˜ì›",
                "ë³‘ì›",
                "í•œì˜ì›",
                "ì¹˜ê³¼",
                "ì•½êµ­",
                "ë³´ê±´",
                "ì˜ë£Œ",
                "í´ë¦¬ë‹‰",
            ]
        ):
            return "ë³´ê±´ì—…"

        if any(
            kw in ì—…ì¢…ëª… for kw in ["ìˆ™ë°•", "í˜¸í…”", "ëª¨í…”", "íœì…˜", "ìš•íƒ•ì—…", "ì°œì§ˆë°©"]
        ):
            return "ìˆ™ë°•ì—…"

        if any(
            kw in ì—…ì¢…ëª…
            for kw in [
                "ì„œë¹„ìŠ¤",
                "ë¯¸ìš©",
                "ì„¸íƒ",
                "ìˆ˜ë¦¬",
                "ì²­ì†Œ",
                "ê´€ë¦¬",
                "ê²½ë¹„",
                "ê²½í˜¸",
                "ì¸í…Œë¦¬ì–´",
                "ë””ìì¸",
                "ë³µì§€",
                "ì‚¬íšŒ",
            ]
        ):
            return "ì„œë¹„ìŠ¤ì—…"

        if any(kw in ì—…ì¢…ëª… for kw in ["ë¶€ë™ì‚°", "ì„ëŒ€", "ì¤‘ê°œ", "ëŒ€ë¦¬", "ë¹Œë”©"]):
            return "ë¶€ë™ì‚°ì—…"

        if any(kw in ì—…ì¢…ëª… for kw in ["ê±´ì„¤", "ê±´ì¶•", "ì‹œê³µ", "í† ëª©"]):
            return "ê±´ì„¤ì—…"

        if any(
            kw in ì—…ì¢…ëª…
            for kw in [
                "ìš´ìˆ˜",
                "ìš´ì†¡",
                "ë°°ì†¡",
                "íƒë°°",
                "ë¬¼ë¥˜",
                "í™”ë¬¼",
                "ìë™ì°¨ ìš´ì†¡",
                "ì°½ê³ ",
            ]
        ):
            return "ìš´ìˆ˜/ë¬¼ë¥˜ì—…"

        if any(
            kw in ì—…ì¢…ëª… for kw in ["ì˜í™”ê´€", "ê·¹ì¥", "ê³µì—°", "ì˜¤ë½", "ì²´ìœ¡", "ë ˆì €"]
        ):
            return "ë¬¸í™”/ì—¬ê°€ì—…"

        return "ê¸°íƒ€"

    @staticmethod
    def map_ë¶„ë¥˜_to_ì—…íƒœ(ëŒ€ë¶„ë¥˜, ì¤‘ë¶„ë¥˜):
        """ëŒ€ë¶„ë¥˜ì™€ ì¤‘ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ì—…íƒœ ë§¤í•‘"""
        if pd.isna(ëŒ€ë¶„ë¥˜):
            return None

        ëŒ€ë¶„ë¥˜ = str(ëŒ€ë¶„ë¥˜).strip()
        ì¤‘ë¶„ë¥˜ = str(ì¤‘ë¶„ë¥˜).strip() if not pd.isna(ì¤‘ë¶„ë¥˜) else ""

        if ëŒ€ë¶„ë¥˜ == "ìš”ì‹ì—…ì²´":
            return "ìŒì‹ì ì—…"

        if ëŒ€ë¶„ë¥˜ == "ê°€ì •ì§‘":
            return "ê°€ì •/ì£¼ê±°"

        if ëŒ€ë¶„ë¥˜ == "ì¼ë°˜ì‚¬ì—…ì²´":
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ê³µì¥", "ì œì¡°"]):
                return "ì œì¡°ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["êµìœ¡", "í•™ì›", "í›ˆë ¨"]):
                return "êµìœ¡ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ì˜ë£Œ", "ë³‘ì›", "ë³´ê±´"]):
                return "ë³´ê±´ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ìˆ™ë°•", "í˜¸í…”"]):
                return "ìˆ™ë°•ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["íŒë§¤", "ìœ í†µ", "ë§ˆíŠ¸", "ìƒê°€"]):
                return "ë„ì†Œë§¤"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ë¹Œë”©", "ë¶€ë™ì‚°"]):
                return "ë¶€ë™ì‚°ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ë³µì§€", "ì‚¬íšŒ"]):
                return "ì„œë¹„ìŠ¤ì—…"
            if any(kw in ì¤‘ë¶„ë¥˜ for kw in ["ì°½ê³ ", "ë¬¼ë¥˜"]):
                return "ìš´ìˆ˜/ë¬¼ë¥˜ì—…"
            if "ì„œë¹„ìŠ¤" in ì¤‘ë¶„ë¥˜:
                return "ì„œë¹„ìŠ¤ì—…"
            return "ê¸°íƒ€ì‚¬ì—…ì²´"

        return "ê¸°íƒ€"

    def map_ì—…ì¢…ëª…_to_ë¶„ë¥˜(self, ì—…ì¢…ëª…_value):
        """ì—…ì¢…ëª… â†’ ëŒ€/ì¤‘/ì†Œ/ì„¸ë¶„ë¥˜ ë§¤í•‘"""

        if pd.isna(ì—…ì¢…ëª…_value):
            return {"ëŒ€ë¶„ë¥˜": None, "ì¤‘ë¶„ë¥˜": None, "ì†Œë¶„ë¥˜": None, "ì„¸ë¶„ë¥˜": None}

        ì—…ì¢…ëª… = str(ì—…ì¢…ëª…_value).strip()

        # 1. ì—…íƒœ ì§ì ‘ ë§¤ì¹­
        if ì—…ì¢…ëª… in self.ì—…íƒœ_to_ë¶„ë¥˜:
            return self.ì—…íƒœ_to_ë¶„ë¥˜[ì—…ì¢…ëª…].copy()

        # 2. ì¢…ëª© ì§ì ‘ ë§¤ì¹­
        if ì—…ì¢…ëª… in self.ì¢…ëª©_to_ë¶„ë¥˜:
            return self.ì¢…ëª©_to_ë¶„ë¥˜[ì—…ì¢…ëª…].copy()

        # 3. ì—…íƒœ ë¶€ë¶„ ë§¤ì¹­
        best_match = None
        best_length = 0
        for ì—…íƒœ, ë¶„ë¥˜ in self.ì—…íƒœ_to_ë¶„ë¥˜.items():
            if len(ì—…íƒœ) >= 2:
                if ì—…íƒœ in ì—…ì¢…ëª… and len(ì—…íƒœ) > best_length:
                    best_match = ë¶„ë¥˜
                    best_length = len(ì—…íƒœ)
                elif len(ì—…ì¢…ëª…) >= 4 and ì—…ì¢…ëª… in ì—…íƒœ and len(ì—…ì¢…ëª…) > best_length:
                    best_match = ë¶„ë¥˜
                    best_length = len(ì—…ì¢…ëª…)

        if best_match is not None:
            return best_match.copy()

        # 4. ì¢…ëª© ë¶€ë¶„ ë§¤ì¹­
        best_match = None
        best_length = 0
        for ì¢…ëª©, ë¶„ë¥˜ in self.ì¢…ëª©_to_ë¶„ë¥˜.items():
            if ì¢…ëª© and len(ì¢…ëª©) >= 2:
                if ì¢…ëª© in ì—…ì¢…ëª… and len(ì¢…ëª©) > best_length:
                    best_match = ë¶„ë¥˜
                    best_length = len(ì¢…ëª©)
                elif len(ì—…ì¢…ëª…) >= 4 and ì—…ì¢…ëª… in ì¢…ëª© and len(ì—…ì¢…ëª…) > best_length:
                    best_match = ë¶„ë¥˜
                    best_length = len(ì—…ì¢…ëª…)

        if best_match is not None:
            return best_match.copy()

        return {"ëŒ€ë¶„ë¥˜": None, "ì¤‘ë¶„ë¥˜": None, "ì†Œë¶„ë¥˜": None, "ì„¸ë¶„ë¥˜": None}

    @staticmethod
    def categorize_area(area):
        if not isinstance(area, (int, float)):
            return 0
        """Bin area into 6 categories"""
        if area <= 22:
            return 0
        elif area <= 50:
            return 1
        elif area <= 258:
            return 2
        elif area <= 1600:
            return 3
        elif area <= 4950:
            return 4
        else:
            return 5

    def recommend(self, customer_data):
        """Generate recommendations based on 8 features with PCA transformation"""

        # Step 1: Encode ëŒ€/ì¤‘/ì†Œ/ì„¸ë¶„ë¥˜
        encoded_ë¶„ë¥˜ = []
        for encoder, col_name in [
            (self.le_ëŒ€ë¶„ë¥˜, "ëŒ€ë¶„ë¥˜"),
            (self.le_ì¤‘ë¶„ë¥˜, "ì¤‘ë¶„ë¥˜"),
            (self.le_ì†Œë¶„ë¥˜, "ì†Œë¶„ë¥˜"),
            (self.le_ì„¸ë¶„ë¥˜, "ì„¸ë¶„ë¥˜"),
        ]:
            val = customer_data.get(col_name, "Unknown")
            if val in encoder.classes_:
                encoded_ë¶„ë¥˜.append(encoder.transform([val])[0])
            else:
                encoded_ë¶„ë¥˜.append(0)

        # Step 2: Apply PCA transformation to get ë¶„ë¥˜_PCA1 and ë¶„ë¥˜_PCA2
        ë¶„ë¥˜_pca = self.pca.transform([encoded_ë¶„ë¥˜])[0]

        # Step 3: Build feature vector with PCA components
        features = {}
        for col in self.feature_cols:
            if col == "í‰ê· _ë©´ì _category":
                area = customer_data.get("í‰ê· _ë©´ì ", 50)
                features[col] = self.categorize_area(area)
            elif col == "ì‹œë„_encoded":
                val = customer_data.get("ì‹œë„ëª…", "Unknown")
                features[col] = (
                    self.le_sido.transform([val])[0]
                    if val in self.le_sido.classes_
                    else 0
                )
            elif col == "ì‹œêµ°êµ¬_encoded":
                val = customer_data.get("ì‹œêµ°êµ¬ëª…", "Unknown")
                features[col] = (
                    self.le_sigungu.transform([val])[0]
                    if val in self.le_sigungu.classes_
                    else 0
                )
            elif col == "ë¶„ë¥˜_PCA1":
                features[col] = ë¶„ë¥˜_pca[0]
            elif col == "ë¶„ë¥˜_PCA2":
                features[col] = ë¶„ë¥˜_pca[1]
            elif col == "ì—…íƒœ_encoded":
                val = customer_data.get("ì—…íƒœ", "Unknown")
                features[col] = (
                    self.le_ì—…íƒœ.transform([val])[0]
                    if val in self.le_ì—…íƒœ.classes_
                    else 0
                )
            elif col in customer_data.index:
                features[col] = customer_data[col]
            else:
                features[col] = 0

        X_new = np.array([features[col] for col in self.feature_cols]).reshape(1, -1)
        X_new_scaled = self.scaler.transform(X_new)
        cluster_id = self.kmeans.predict(X_new_scaled)[0]

        cluster_recs = self.cluster_recommendations[cluster_id]
        return {
            "cluster": cluster_id,
            "contract_recommendations": cluster_recs.get(
                "contract_recommendations", []
            ),
            "product_recommendations": cluster_recs.get("product_recommendations", []),
            "X_scaled": X_new_scaled,
            "pca_components": ë¶„ë¥˜_pca,
        }

    def run_inference(self, inference_df, output_dir="output_pca"):
        """Run inference on new customers using PCA model"""

        print("=" * 60)
        print("Simple Customer Segmentation - Inference (PCA Model)")
        print("=" * 60)

        # Load model artifacts
        print("\nğŸ“¦ Loading PCA model artifacts...")
        self.load_model_artifacts()

        print(f"âœ… Loaded {len(inference_df)} customers")

        # Extract location features
        print("\nğŸ—ºï¸  Extracting location features...")
        if "ì£¼ì†Œ1" in inference_df.columns and "ì‹œë„ëª…" not in inference_df.columns:
            inference_df["ì‹œë„ëª…"] = inference_df["ì£¼ì†Œ1"].apply(self.extract_sido)
        if "ì£¼ì†Œ1" in inference_df.columns and "ì‹œêµ°êµ¬ëª…" not in inference_df.columns:
            inference_df["ì‹œêµ°êµ¬ëª…"] = inference_df["ì£¼ì†Œ1"].apply(self.extract_sigungu)

        if (
            "ì—…ì¢…ëª…" not in inference_df.columns
            and "í‘œì¤€ì‚°ì—…ì½”ë“œ" in inference_df.columns
        ):
            inference_df["ì—…ì¢…ëª…"] = inference_df["í‘œì¤€ì‚°ì—…ì½”ë“œ"].apply(
                self.map_code_to_name
            )

        # Map ì—…ì¢…ëª… â†’ ëŒ€/ì¤‘/ì†Œ/ì„¸ë¶„ë¥˜ â†’ ì—…íƒœ
        if "ì—…ì¢…ëª…" in inference_df.columns:
            print("ğŸ¯ ì—…ì¢…ëª… â†’ ëŒ€/ì¤‘/ì†Œ/ì„¸ë¶„ë¥˜ ë§¤í•‘ ì¤‘...")

            ë¶„ë¥˜_results = inference_df["ì—…ì¢…ëª…"].apply(self.map_ì—…ì¢…ëª…_to_ë¶„ë¥˜)
            inference_df["ëŒ€ë¶„ë¥˜"] = ë¶„ë¥˜_results.apply(lambda x: x["ëŒ€ë¶„ë¥˜"])
            inference_df["ì¤‘ë¶„ë¥˜"] = ë¶„ë¥˜_results.apply(lambda x: x["ì¤‘ë¶„ë¥˜"])
            inference_df["ì†Œë¶„ë¥˜"] = ë¶„ë¥˜_results.apply(lambda x: x["ì†Œë¶„ë¥˜"])
            inference_df["ì„¸ë¶„ë¥˜"] = ë¶„ë¥˜_results.apply(lambda x: x["ì„¸ë¶„ë¥˜"])

            inference_df["ì—…íƒœ_from_ë¶„ë¥˜"] = inference_df.apply(
                lambda row: self.map_ë¶„ë¥˜_to_ì—…íƒœ(row["ëŒ€ë¶„ë¥˜"], row["ì¤‘ë¶„ë¥˜"]), axis=1
            )
            inference_df["ì—…íƒœ_from_í‚¤ì›Œë“œ"] = inference_df["ì—…ì¢…ëª…"].apply(
                self.map_ì—…ì¢…ëª…_to_ì—…íƒœ
            )
            inference_df["ì—…íƒœ"] = inference_df["ì—…íƒœ_from_ë¶„ë¥˜"].fillna(
                inference_df["ì—…íƒœ_from_í‚¤ì›Œë“œ"]
            )

            mapped_count = inference_df["ëŒ€ë¶„ë¥˜"].notna().sum()
            print(f"âœ… ì—…ì¢…ëª… â†’ ë¶„ë¥˜ ë§¤í•‘: {mapped_count}/{len(inference_df)}")
            print(
                f"âœ… ìµœì¢… ì—…íƒœ ë§¤í•‘: {inference_df['ì—…íƒœ'].notna().sum()}/{len(inference_df)}"
            )

        elif "ëŒ€ë¶„ë¥˜" in inference_df.columns and "ì¤‘ë¶„ë¥˜" in inference_df.columns:
            print("ğŸ¯ Using existing classification data...")
            inference_df["ì—…íƒœ"] = inference_df.apply(
                lambda row: self.map_ë¶„ë¥˜_to_ì—…íƒœ(row["ëŒ€ë¶„ë¥˜"], row["ì¤‘ë¶„ë¥˜"]), axis=1
            )
            print(
                f"âœ… Classification-based: {inference_df['ì—…íƒœ'].notna().sum()}/{len(inference_df)}"
            )

        elif "ì—…ì¢…ëª…" in inference_df.columns:
            print("âš ï¸  No mapping tables, using keyword-based mapping...")
            inference_df["ì—…íƒœ"] = inference_df["ì—…ì¢…ëª…"].apply(self.map_ì—…ì¢…ëª…_to_ì—…íƒœ)
            print(
                f"âœ… Mapped ì—…íƒœ: {inference_df['ì—…íƒœ'].notna().sum()}/{len(inference_df)}"
            )

        # Generate recommendations
        print("\nğŸ”® Generating recommendations with PCA model...")
        inference_output_rows = []

        # Calculate max distance for similarity score
        X_scaled_train = self.scaler.transform(
            self.clustering_df[self.feature_cols].values
        )
        max_dist = np.max(self.kmeans.transform(X_scaled_train))

        for i, (idx, customer) in enumerate(inference_df.iterrows()):
            if (i + 1) % 50 == 0:
                print(f"  Processing {i + 1}/{len(inference_df)}...")

            # Get first contract to exclude from recommendations
            first_contract_to_exclude = None
            cust_code = customer.get("ê³ ê°ì½”ë“œ", "")
            if cust_code:
                ground_truth_row = inference_df[inference_df["ê³ ê°ì½”ë“œ"] == cust_code]
                if not ground_truth_row.empty:
                    contracts_info_json = ground_truth_row.iloc[0].get(
                        "contracts_info", ""
                    )
                    if pd.notna(contracts_info_json) and contracts_info_json:
                        try:
                            contracts_list = json.loads(contracts_info_json)
                            contracts_with_dates = []
                            for c in contracts_list:
                                contract_target = c.get("ê³„ì•½ëŒ€ìƒ", "")
                                contract_date = c.get("ê³„ì•½ì¼ì", "")
                                if contract_target and contract_date:
                                    try:
                                        date_obj = pd.to_datetime(contract_date)
                                        contracts_with_dates.append(
                                            {
                                                "target": contract_target,
                                                "date": date_obj,
                                            }
                                        )
                                    except Exception:
                                        contracts_with_dates.append(
                                            {
                                                "target": contract_target,
                                                "date": pd.Timestamp.max,
                                            }
                                        )
                            if contracts_with_dates:
                                contracts_with_dates.sort(key=lambda x: x["date"])
                                first_contract_to_exclude = contracts_with_dates[0][
                                    "target"
                                ]
                        except (json.JSONDecodeError, TypeError):
                            pass

            # Prepare customer data
            customer_data = pd.Series(
                {
                    "ìœ„ë„": customer.get("ìœ„ë„", 37.5)
                    if pd.notna(customer.get("ìœ„ë„"))
                    else 37.5,
                    "ê²½ë„": customer.get("ê²½ë„", 127.0)
                    if pd.notna(customer.get("ê²½ë„"))
                    else 127.0,
                    "ì‹œë„ëª…": customer.get("ì‹œë„ëª…", "Unknown"),
                    "ì‹œêµ°êµ¬ëª…": customer.get("ì‹œêµ°êµ¬ëª…", "Unknown"),
                    "ëŒ€ë¶„ë¥˜": customer.get("ëŒ€ë¶„ë¥˜", "Unknown"),
                    "ì¤‘ë¶„ë¥˜": customer.get("ì¤‘ë¶„ë¥˜", "Unknown"),
                    "ì†Œë¶„ë¥˜": customer.get("ì†Œë¶„ë¥˜", "Unknown"),
                    "ì„¸ë¶„ë¥˜": customer.get("ì„¸ë¶„ë¥˜", "Unknown"),
                    "ì—…íƒœ": customer.get("ì—…íƒœ", "Unknown"),
                    "í‰ê· _ë©´ì ": customer.get("ê±´ë¬¼ê·œëª¨", 50)
                    if pd.notna(customer.get("ê±´ë¬¼ê·œëª¨"))
                    else 50,
                }
            )

            # Get recommendations (with PCA transformation)
            recs = self.recommend(customer_data)

            # Calculate similarity
            cluster_id = recs["cluster"]
            X_new_scaled = recs["X_scaled"]
            distance = np.linalg.norm(
                X_new_scaled - self.kmeans.cluster_centers_[cluster_id]
            )
            similarity_score = 1 - (distance / max_dist)

            contract_recs_all = recs["contract_recommendations"][
                :10
            ]  # Get top 10 to re-sort
            product_recs_all = recs["product_recommendations"][:10]
            # Find most similar customer in cluster (closest by distance)
            cluster_data = self.clustering_df[
                self.clustering_df["cluster_pca"] == cluster_id
            ]
            cluster_size = len(cluster_data)

            contract_usage_data = []
            for contract in contract_recs_all:
                # Skip the first contract
                if contract == first_contract_to_exclude:
                    continue
                # Count unique customers who have this contract
                unique_customers = set()
                for _, row in cluster_data.iterrows():
                    if contract in row["ê³„ì•½ì½”ë“œ_ë¦¬ìŠ¤íŠ¸"]:
                        unique_customers.add(row["ê³ ê°ì½”ë“œ"])
                usage_rate = (
                    (len(unique_customers) / cluster_size * 100)
                    if cluster_size > 0
                    else 0
                )
                contract_usage_data.append((contract, usage_rate))

            # Sort by usage rate (descending) and take top 3
            contract_usage_data.sort(key=lambda x: x[1], reverse=True)
            contract_recs = [c for c, _ in contract_usage_data[:3]]
            contract_usage_rates = [r for _, r in contract_usage_data[:3]]

            # Calculate purchase rates for ALL product recommendations (DEDUPLICATED)
            product_purchase_data = []
            for product in product_recs_all:
                # Count unique customers who purchased this product
                unique_customers = set()
                for _, row in cluster_data.iterrows():
                    if product in row["ë§ˆì´ë©_ìƒí’ˆëª…_ë¦¬ìŠ¤íŠ¸"]:
                        unique_customers.add(row["ê³ ê°ì½”ë“œ"])
                purchase_rate = (
                    (len(unique_customers) / cluster_size * 100)
                    if cluster_size > 0
                    else 0
                )
                product_purchase_data.append((product, purchase_rate))

            # Sort by purchase rate (descending) and take top 3
            product_purchase_data.sort(key=lambda x: x[1], reverse=True)
            product_recs = [p for p, _ in product_purchase_data[:3]]
            product_purchase_rates = [r for _, r in product_purchase_data[:3]]

            if len(cluster_data) > 0:
                # Calculate distance to all customers in cluster
                cluster_features = cluster_data[self.feature_cols].values
                cluster_scaled = self.scaler.transform(cluster_features)

                # Find closest customer
                distances = np.linalg.norm(cluster_scaled - X_new_scaled, axis=1)
                closest_idx = np.argmin(distances)
                similar_customer = cluster_data.iloc[closest_idx]
            else:
                similar_customer = None

            if self.inference_mode == "current_user":
                row = {
                    "CCOD": customer.get("ê³ ê°ì½”ë“œ", ""),
                    "snapshot_month": datetime.now().strftime("%Y-%m-01"),
                    "user_cluster": cluster_id,
                    "cluster_similarity": similarity_score,
                    "sim_CCOD": similar_customer["ê³ ê°ì½”ë“œ"]
                    if similar_customer is not None
                    else "",
                    "sim_user_name": similar_customer["ê³ ê°ëª…"]
                    if similar_customer is not None
                    else "",
                    "sim_user_contracts": json.dumps(
                        list(set(similar_customer["ê³„ì•½ì½”ë“œ_ë¦¬ìŠ¤íŠ¸"])),
                        ensure_ascii=False,
                    )
                    if similar_customer is not None
                    and isinstance(similar_customer.get("ê³„ì•½ì½”ë“œ_ë¦¬ìŠ¤íŠ¸"), list)
                    else "[]",  # JSONB
                    "sim_user_products": json.dumps(
                        list(set(similar_customer["ë§ˆì´ë©_ìƒí’ˆëª…_ë¦¬ìŠ¤íŠ¸"])),
                        ensure_ascii=False,
                    )
                    if similar_customer is not None
                    and isinstance(similar_customer.get("ë§ˆì´ë©_ìƒí’ˆëª…_ë¦¬ìŠ¤íŠ¸"), list)
                    else "[]",  # JSONB
                }
            else:
                row = {
                    "BZNO": customer.get("ì‚¬ì—…ìë²ˆí˜¸", ""),
                    "ENP_NM": customer.get("ìƒí˜¸ëª…", ""),
                    "KEDCD": customer.get("KEDCD", ""),
                    "BZPL_CD": customer.get("BZPL_CD", ""),
                    "BZPL_SEQ": customer.get("BZPL_SEQ", ""),
                    "ENP_NP": customer.get("ENP_NP", ""),
                    "snapshot_month": datetime.now().strftime("%Y-%m-01"),
                    "user_cluster": cluster_id,
                    "cluster_similarity": similarity_score,
                    "sim_CCOD": similar_customer["ê³ ê°ì½”ë“œ"]
                    if similar_customer is not None
                    else "",
                    "sim_user_name": similar_customer["ê³ ê°ëª…"]
                    if similar_customer is not None
                    else "",
                    "sim_user_contracts": json.dumps(
                        list(set(similar_customer["ê³„ì•½ì½”ë“œ_ë¦¬ìŠ¤íŠ¸"])),
                        ensure_ascii=False,
                    )
                    if similar_customer is not None
                    and isinstance(similar_customer.get("ê³„ì•½ì½”ë“œ_ë¦¬ìŠ¤íŠ¸"), list)
                    else "[]",  # JSONB
                    "sim_user_products": json.dumps(
                        list(set(similar_customer["ë§ˆì´ë©_ìƒí’ˆëª…_ë¦¬ìŠ¤íŠ¸"])),
                        ensure_ascii=False,
                    )
                    if similar_customer is not None
                    and isinstance(similar_customer.get("ë§ˆì´ë©_ìƒí’ˆëª…_ë¦¬ìŠ¤íŠ¸"), list)
                    else "[]",  # JSONB
                }

            # Add recommendations with usage rate
            for j in range(3):
                if j < len(contract_recs):
                    contract = contract_recs[j]
                    usage_rate = contract_usage_rates[j]
                    row[f"rec_contract_{j+1}"] = contract
                    row[f"rec_contract_{j+1}_reason"] = (
                        f"ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë‚´ {usage_rate:.1f}%ê°€ ì´ìš©ì¤‘"
                    )
                else:
                    row[f"rec_contract_{j+1}"] = ""
                    row[f"rec_contract_{j+1}_reason"] = ""

            for j in range(3):
                if j < len(product_recs):
                    product = product_recs[j]
                    purchase_rate = product_purchase_rates[j]
                    row[f"rec_product_{j+1}"] = product
                    row[f"rec_product_{j+1}_reason"] = (
                        f"ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë‚´ {purchase_rate:.1f}%ê°€ êµ¬ë§¤í•¨"
                    )
                else:
                    row[f"rec_product_{j+1}"] = ""
                    row[f"rec_product_{j+1}_reason"] = ""

            inference_output_rows.append(row)

        # Save results
        print("\nğŸ’¾ Saving results...")
        os.makedirs(output_dir, exist_ok=True)
        inference_output_df = pd.DataFrame(inference_output_rows)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"{output_dir}/output_potential_users_recommendations_pca_{timestamp}_known.csv"
        inference_output_df.to_csv(output_path, index=False, encoding="utf-8-sig")

        if self.inference_mode == "current_user":
            ensure_partition_exists(
                schema="analytics",
                table_name="user_recommendation",
                target_date=datetime.now(),
            )
            fast_bulk_insert(
                inference_output_df,
                table_name="user_recommendation",
                schema="analytics",
            )
        else:
            ensure_partition_exists(
                schema="analytics",
                table_name="potential_user_recommendation",
                target_date=datetime.now(),
            )
            fast_bulk_insert(
                inference_output_df,
                table_name="potential_user_recommendation",
                schema="analytics",
            )

        print(f"\n{'='*60}")
        print("âœ… Inference complete!")
        print(f"{'='*60}")
        print("ğŸ“Š Results:")
        print(f"   Total customers: {len(inference_output_df)}")
        print(f"   Clusters used: {inference_output_df['user_cluster'].nunique()}")
        print(f"   Output saved: {output_path}")
        print("\nğŸ“ˆ Cluster distribution:")
        print(inference_output_df["user_cluster"].value_counts().sort_index())
        print("\nğŸ’¯ Similarity scores:")
        print(f"   Mean: {inference_output_df['cluster_similarity'].mean():.2%}")
        print(f"   Median: {inference_output_df['cluster_similarity'].median():.2%}")
        print(f"   Min: {inference_output_df['cluster_similarity'].min():.2%}")
        print(f"   Max: {inference_output_df['cluster_similarity'].max():.2%}")

        return inference_output_df


if __name__ == "__main__":
    recommender = UserRecommender()
    recommender.update_cluster_info(snapshot_month="2025_11")
    # recommender.run_inference(pd.read_csv('./user_contract_data.csv'), output_dir='.')
