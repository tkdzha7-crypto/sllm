import json
import re
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

import numpy as np
import pandas as pd
import requests
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class VoCAnalyzer:
    def __init__(self, db_connection, api_url: str = "http://172.16.3.220:8000"):
        self.voc_categories_df = None
        self.categories = None
        self.api_url = api_url
        self.db_connection = db_connection

        # Predefined bug list for TF-IDF matching (kept from ingest.py)
        self.BUG_LIST = [
            "Ï•ê",
            "Î∞îÌÄ¥",
            "Í∞úÎØ∏",
            "Ï†ÄÍ≥°Ìï¥Ï∂©",
            "ÏßÑÎìúÍ∏∞",
            "Î®ºÏßÄÎã§Îì¨Ïù¥",
            "Í≥†ÏñëÏù¥",
            "Í±∞ÎØ∏",
            "ÏßëÍ≤åÎ≤åÎ†à",
            "ÌååÎ¶¨",
            "Î™®Í∏∞",
            "ÌïòÎ£®ÏÇ¥Ïù¥",
            "ÌôîÎûëÍ≥°ÎÇòÎ∞©",
            "ÎÇòÎ∞©",
            "ÎØ∏ÎèôÏ†ï",
            "Í∏∞ÌÉÄ",
            "ÏßëÏõÖÏ•ê",
            "ÏãúÍ∂ÅÏ•ê",
            "ÏÉùÏ•ê",
            "ÎïÉÏ•ê",
            "ÎëêÎçîÏßÄ",
            "Îì§Ï•ê",
            "Ìù∞ÎÑìÏ†ÅÎã§Î¶¨Î∂âÏùÄÏ•ê",
            "Îì±Ï§ÑÏ•ê",
            "ÎèÖÏùºÎ∞îÌÄ¥",
            "ÎØ∏Íµ≠Î∞îÌÄ¥",
            "Î®πÎ∞îÌÄ¥",
            "ÏùºÎ≥∏Î∞îÌÄ¥",
            "Í≤ΩÎèÑÎ∞îÌÄ¥",
            "ÏÇ∞Î∞îÌÄ¥",
            "Ïï†ÏßëÍ∞úÎØ∏",
            "Ïπ®Í∞úÎØ∏",
            "Ïú†Î†πÍ∞úÎØ∏",
            "ÎØ∏ÏπúÍ∞úÎØ∏",
            "ÎØ∏ÎèôÏ†ïÍ∞úÎØ∏",
            "Ïô∏Í≥ΩÍ∞úÎØ∏",
            "ÏåÄÎ∞îÍµ¨ÎØ∏",
            "Ìå•Î∞îÍµ¨ÎØ∏",
            "Í±∞ÏßìÏåÄÎèÑÎëëÍ±∞Ï†ÄÎ¶¨",
            "ÌÜ±Í∞ÄÏä¥Î®∏Î¶¨ÎåÄÏû•",
            "Ïû•Îëê",
            "Ïï†ÏïåÎùΩÏàòÏãúÎ†ÅÏù¥",
            "Ïï†ÏàòÏãúÎ†ÅÏù¥",
            "ÏïîÍ≤ÄÏùÄÏàòÏãúÎ†ÅÏù¥",
            "Í∂åÏó∞Î≤åÎ†à",
            "Í±∞Ï†ÄÎ¶¨",
            "Ìù°ÌòàÏßÑÎìúÍ∏∞",
            "Ï•êÎ©∞ÎäêÎ¶¨",
            "ÎÖ∏ÎûòÍ∏∞",
            "ÏßÄÎÑ§",
            "ÏñºÎ£©Ï†êÏ¥àÌååÎ¶¨",
            "ÏßëÌååÎ¶¨",
            "Îî∏ÏßëÌååÎ¶¨",
            "ÎÇòÎ∞©ÌååÎ¶¨",
            "Ï¥àÌååÎ¶¨",
            "ÏñºÎ£©Î¨¥Îä¨Îì±Ï¥àÌååÎ¶¨",
            "ÎÇ†ÌååÎ¶¨",
            "Í≤ÄÏ†ïÎÇ†Í∞úÎ≤ÑÏÑØÌååÎ¶¨Í≥º",
            "Î≤ºÎ£©ÌååÎ¶¨",
            "ÌÅ∞Í≤ÄÏ†ïÌååÎ¶¨",
            "Íµ¨Î¶¨Í∏àÌååÎ¶¨",
            "Ïâ¨ÌååÎ¶¨",
            "Ïï†Í∏∞Îò•ÌååÎ¶¨",
            "Î∂âÏùÄÎì±Ïö∞Îã®ÌÑ∏ÌååÎ¶¨",
            "Ïà≤Î™®Í∏∞",
            "ÏßÄÌïòÏßëÎ™®Í∏∞",
            "Ïû•Íµ¨Î≤åÎ†à",
            "ÍπîÎî∞Íµ¨",
            "ÍπîÎî∞Íµ¨Ïú†Ï∂©",
            "Ï§ÑÏïåÎùΩÎ™ÖÎÇòÎ∞©",
            "ÏßÄÏ§ëÌï¥Í∞ÄÎ£®Î™ÖÎÇòÎ∞©",
            "Î©∏Í∞ïÎÇòÎ∞©",
            "Ìï¥Ï∂©ÏóÜÏùå",
            "Îî±Ï†ïÎ≤åÎ†à",
            "ÌíçÎéÖÏù¥",
            "Î®ºÏßÄÎ≤åÎ†à",
            "Î©îÎöúÍ∏∞",
            "Î∞©ÏïÑÍπ®ÎπÑ",
            "Ïó¨Ïπò",
            "Îß§ÎØ∏",
            "ÌÑ∏ÌååÎ¶¨",
            "ÌïòÎäòÏÜå",
            "Îì±Ïóê",
            "ÍºΩÎì±Ïù¥",
            "Î≤å",
            "Ìù∞Í∞úÎØ∏",
            "Î©∏Íµ¨",
            "Í∞ÅÎã§Í∑Ä",
            "ÏÇ¨Î©¥Î∞úÏù¥",
            "ÌÜ°ÌÜ†Í∏∞",
            "Î¨ºÏûêÎùº",
            "Î∞òÎÇ†Í∞ú",
            "ÌíÄÏû†ÏûêÎ¶¨",
            "ÏÇ¨Ïä¥Î≤åÎ†à",
            "Î¨¥ÎãπÎ≤åÎ†à",
            "Ìù°ÌòàÌï¥Ï∂©",
            "Î≤ºÎ£©",
            "ÏùëÏï†",
            "ÎπàÎåÄ",
            "Î≥¥Î¶¨ÎÇòÎ∞©",
            "Í∑∏Î¶¨Îßà",
            "Í∑ÄÎöúÎùºÎØ∏",
            "ÏßÅÎ¨ºÌï¥Ï∂©",
            "Ïò∑Ï¢ÄÎÇòÎ∞©",
            "Ï¢ÄÎ≤åÎ†à",
            "ÏàòÎ™©Ìï¥Ï∂©",
            "ÏßÑÎîßÎ¨º",
            "ÏÜ°Ï∂©Ïù¥",
            "ÎÖ∏Î¶∞Ïû¨",
            "Î™©Ïû¨Ìï¥Ï∂©",
            "Í∏∏ÏïûÏû°Ïù¥",
            "ÎØ∏Íµ≠ÏÑ†ÎÖÄÎ≤åÎ†à",
            "ÎÇ†ÎèÑÎûòÎ•ò",
            "ÎèôÍµ¥ÌëúÎ≥∏Î≤åÎ†à",
            "Î∞©ÏïÑÎ≤åÎ†à",
            "ÎÇòÎ¨¥Ï¢ÄÎ≤åÎ†à",
            "Í∂åÎ†®Ïπ®Î≤å",
            "Í∞ÄÎ£®Ïù¥",
            "Îß§ÎØ∏Ï∂©",
            "Îã§Îì¨Ïù¥Î≤åÎ†à(Ïú†ÏãúÏ∂©)",
            "Ïç¨ÎçîÎ∏îÎ£® Ìè¨Ìöç",
            "Î∏îÎ£®Ïä§ÌÜ∞ Ìè¨Ìöç",
        ]

        # Category name mappings (from ingest.py)
        self.category_mappings = {
            "ÎπÑÏö©/Í≥ÑÏïΩ Î¨∏Ï†ú": "ÏöîÍ∏à/Í≥ÑÏïΩ Î¨∏Ï†ú",
            "Í≥ÑÏïΩ/ÎπÑÏö© Î¨∏Ï†ú": "ÏöîÍ∏à/Í≥ÑÏïΩ Î¨∏Ï†ú",
            "ÏöîÍ∏à Î¨∏Ï†ú": "ÏöîÍ∏à/Í≥ÑÏïΩ Î¨∏Ï†ú",
            "Í≥ÑÏïΩ Î¨∏Ï†ú": "ÏöîÍ∏à/Í≥ÑÏïΩ Î¨∏Ï†ú",
            "Ìï¥Ï∂©": "Ìï¥Ï∂© Î¨∏Ï†ú",
            "Ï†úÌíà Î¨∏Ï†ú": "Ï†úÌíà",
            "ÏÑúÎπÑÏä§": "ÏÑúÎπÑÏä§ ÌíàÏßà",
            "ÏÑúÎπÑÏä§ÌíàÏßà": "ÏÑúÎπÑÏä§ ÌíàÏßà",
            "Î∞∞ÏÜ°": "Î∞∞ÏÜ°Î¨∏Ï†ú",
            "ÏãúÏä§ÌÖú Ïò§Î•ò": "ÏãúÏä§ÌÖú/Ï†ÑÏÇ∞ Ïò§Î•ò",
            "Ï†ÑÏÇ∞ Ïò§Î•ò": "ÏãúÏä§ÌÖú/Ï†ÑÏÇ∞ Ïò§Î•ò",
            "Ïö¥ÏòÅ": "Ïö¥ÏòÅ Í¥ÄÎ¶¨",
        }

    def input_text_cleansing(self, text: str) -> str:
        """Cleanses the input text by removing special characters and extra spaces."""
        if not isinstance(text, str):
            return ""
        # Remove special characters (keep Korean, English, numbers, and basic punctuation)
        cleaned_text = re.sub(r"[^Í∞Ä-Ìû£a-zA-Z0-9\s.,!?]", " ", text)
        # Replace multiple spaces with a single space
        cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
        return cleaned_text

    def normalize_category_name(self, category_name: str) -> str:
        """Normalize category name using mapping table"""
        if not category_name:
            return category_name
        if category_name in self.category_mappings:
            normalized = self.category_mappings[category_name]
            # keep a small debug print here for traceability
            print(f"üîÑ Mapped '{category_name}' ‚Üí '{normalized}'")
            return normalized
        return category_name

    def build_input_categories(self):
        """Fetch voc categories from DB and build the nested input_categories structure.
        Structure: {ÎåÄÎ∂ÑÎ•ò: {Ï§ëÎ∂ÑÎ•ò: [ÏÜåÎ∂ÑÎ•ò_list]}}
        The resulting JSON string is stored in self.categories (ready to send to SLLM API).
        """
        # Resolve engine from provided db_connection
        engine = None
        if (
            hasattr(self.db_connection, "engine")
            and self.db_connection.engine is not None
        ):
            engine = self.db_connection.engine
        else:
            engine = self.db_connection

        if engine is None:
            raise RuntimeError("No valid DB engine available in db_connection")

        query = "SELECT id, voc_id, name, parent_id, level FROM source.voc_category"
        try:
            self.voc_categories_df = pd.read_sql(query, engine)
        except Exception as e:
            print(f"‚ùå Error loading voc_category table: {e}")
            self.voc_categories_df = pd.DataFrame(
                columns=["id", "voc_id", "name", "parent_id", "level"]
            )

        input_categories = {}
        level1_categories = self.voc_categories_df[
            self.voc_categories_df["level"] == 1.0
        ]

        for _, level1_cat in level1_categories.iterrows():
            main_name = level1_cat["name"]
            main_voc_id = level1_cat["voc_id"]

            level2_categories = self.voc_categories_df[
                (self.voc_categories_df["level"] == 2.0)
                & (self.voc_categories_df["parent_id"] == main_voc_id)
            ]

            sub_dict = {}
            for _, level2_cat in level2_categories.iterrows():
                sub_name = level2_cat["name"]
                sub_voc_id = level2_cat["voc_id"]

                level3_categories = self.voc_categories_df[
                    (self.voc_categories_df["level"] == 3.0)
                    & (self.voc_categories_df["parent_id"] == sub_voc_id)
                ]

                detail_list = level3_categories["name"].tolist()
                sub_dict[sub_name] = detail_list

            input_categories[main_name] = sub_dict

        # Store JSON string to send to SLLM
        try:
            self.categories = json.dumps(input_categories, ensure_ascii=False, indent=2)
        except Exception:
            # Fallback to Python dict if JSON dumping fails
            self.categories = input_categories

        print(
            f"‚úÖ Built input_categories structure with {len(input_categories)} main categories"
        )
        return input_categories

    def find_related_bug(self, content_text, threshold=0.3):
        """Find the most related bug using TF-IDF similarity (fallback when SLLM doesn't provide bug_type)."""
        if not content_text or not self.BUG_LIST:
            return None

        try:
            corpus = [content_text] + self.BUG_LIST
            vectorizer = TfidfVectorizer(
                stop_words=None, max_features=1000, ngram_range=(1, 2)
            )
            tfidf_matrix = vectorizer.fit_transform(corpus)
            content_vector = tfidf_matrix[0:1]
            bug_vectors = tfidf_matrix[1:]
            similarities = cosine_similarity(content_vector, bug_vectors).flatten()
            max_similarity = np.max(similarities)
            if max_similarity >= threshold:
                best_bug_index = int(np.argmax(similarities))
                best_bug = self.BUG_LIST[best_bug_index]
                print(
                    f"üîç TF-IDF found related bug: '{best_bug}' (similarity: {max_similarity:.3f})"
                )
                return best_bug
            else:
                print(
                    f"üîç No related bug found above threshold {threshold} (max similarity: {max_similarity:.3f})"
                )
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è Error in TF-IDF bug matching: {e}")
            return None

    def get_category_info(self, category_name, level=None, parent_voc_id=None):
        """Get category id, voc_id, and actual name by name, level, and parent relationship. Falls back to 'Í∏∞ÌÉÄ' when missing."""
        if not category_name:
            return None, None, None

        normalized_name = self.normalize_category_name(category_name)

        if self.voc_categories_df is None:
            print("‚ö†Ô∏è voc_categories_df not loaded; call build_input_categories() first")
            return None, None, None

        matches = self.voc_categories_df[
            self.voc_categories_df["name"] == normalized_name
        ]
        if level is not None:
            matches = matches[matches["level"] == level]
        if parent_voc_id is not None:
            matches = matches[matches["parent_id"] == parent_voc_id]

        if len(matches) > 0:
            match = matches.iloc[0]
            return int(match["id"]), match["voc_id"], match["name"]
        else:
            parent_info = f" under parent {parent_voc_id}" if parent_voc_id else ""
            print(
                f"‚ö†Ô∏è No match found for category: {normalized_name} (original: {category_name}) (level: {level}){parent_info}"
            )
            print(f"üîÑ Falling back to 'Í∏∞ÌÉÄ' category for level {level}")

            Í∏∞ÌÉÄ_matches = self.voc_categories_df[
                self.voc_categories_df["name"] == "Í∏∞ÌÉÄ"
            ]
            if level is not None:
                Í∏∞ÌÉÄ_matches = Í∏∞ÌÉÄ_matches[Í∏∞ÌÉÄ_matches["level"] == level]
            if parent_voc_id is not None:
                Í∏∞ÌÉÄ_matches = Í∏∞ÌÉÄ_matches[Í∏∞ÌÉÄ_matches["parent_id"] == parent_voc_id]

            if len(Í∏∞ÌÉÄ_matches) > 0:
                Í∏∞ÌÉÄ_match = Í∏∞ÌÉÄ_matches.iloc[0]
                print(f"‚úÖ Using Í∏∞ÌÉÄ category: {Í∏∞ÌÉÄ_match['voc_id']}")
                return int(Í∏∞ÌÉÄ_match["id"]), Í∏∞ÌÉÄ_match["voc_id"], Í∏∞ÌÉÄ_match["name"]
            else:
                general_Í∏∞ÌÉÄ_matches = self.voc_categories_df[
                    (self.voc_categories_df["name"] == "Í∏∞ÌÉÄ")
                    & (self.voc_categories_df["level"] == level)
                ]
                if len(general_Í∏∞ÌÉÄ_matches) > 0:
                    Í∏∞ÌÉÄ_match = general_Í∏∞ÌÉÄ_matches.iloc[0]
                    print(f"‚úÖ Using general Í∏∞ÌÉÄ category: {Í∏∞ÌÉÄ_match['voc_id']}")
                    return (
                        int(Í∏∞ÌÉÄ_match["id"]),
                        Í∏∞ÌÉÄ_match["voc_id"],
                        Í∏∞ÌÉÄ_match["name"],
                    )
                else:
                    print(f"‚ö†Ô∏è No Í∏∞ÌÉÄ category found for level {level}")
                    return None, None, None

    def _postprocess_response(self, sample, sllm_response, confidence_score):
        """Process the SLLM response and build the sample_output dict.

        Args:
            sample: A pandas namedtuple with rcno, ccod, msg_id, received_at, content
            sllm_response: The parsed response from SLLM API
            confidence_score: The confidence score from SLLM API

        Returns:
            dict: The processed sample output with all category mappings
        """
        now_kst = datetime.now(ZoneInfo("Asia/Seoul"))

        if sllm_response is None:
            print("‚ö†Ô∏è Received None response from SLLM API, using default values")
            sllm_response = {
                "categories": [],
                "keywords": None,
                "bug_type": None,
                "is_claim": "no_claim",
                "summary": None,
            }
            confidence_score = 0.0

        inferred_categories = sllm_response.get("categories", [])

        keywords_data = sllm_response.get("keywords")
        if keywords_data is not None:
            filtered_keywords = [kw for kw in keywords_data if kw in sample.content]
            keywords_json = (
                json.dumps(filtered_keywords, ensure_ascii=False)
                if not isinstance(filtered_keywords, str)
                else filtered_keywords
            )
        else:
            keywords_json = None

        bug_type = sllm_response.get("bug_type")
        if not bug_type:
            print(
                f"üîç No bug_type from SLLM, using TF-IDF for content: {sample.content[:100]}..."
            )
            bug_type = self.find_related_bug(sample.content)

        # The received_at from source is a naive datetime already in KST
        # Keep it as naive so PostgreSQL stores it as-is without conversion
        msg_received_at = sample.received_at
        if hasattr(msg_received_at, "tzinfo") and msg_received_at.tzinfo is not None:
            # If it has timezone info, strip it to prevent double conversion
            msg_received_at = msg_received_at.replace(tzinfo=None)
        elif isinstance(msg_received_at, pd.Timestamp):
            # Convert pandas Timestamp to Python datetime (naive)
            msg_received_at = msg_received_at.to_pydatetime().replace(tzinfo=None)

        sample_output = {
            "rcno": sample.rcno,
            "ccod": sample.ccod,
            "msg_id": sample.msg_id,
            "msg_received_at": msg_received_at
            - timedelta(hours=9),  # Store as UTC naive
            "created_at": now_kst,
            "updated_at": now_kst,
            "model_name": "cesco_sLLM_Qwen_3",
            "model_ver": "1.0",
            "content": sample.content,
            "is_claim": 1 if sllm_response.get("is_claim") == "claim" else 0,
            "summary": sllm_response.get("summary"),
            "keywords": keywords_json,
            "bug_type": bug_type,
            "model_confidence": confidence_score,
        }

        # Map categories (up to 5 levels like ingest.py)
        # First, resolve all categories to get actual names after fallback
        resolved_categories = []
        for i in range(5):
            category_data = (
                inferred_categories[i] if i < len(inferred_categories) else {}
            )

            main_name = category_data.get("ÎåÄÎ∂ÑÎ•ò")
            sub_name = category_data.get("Ï§ëÎ∂ÑÎ•ò")
            detail_name = category_data.get("ÏÜåÎ∂ÑÎ•ò")
            detail_reason = category_data.get("Í∑ºÍ±∞")

            main_id, main_code, actual_main_name = self.get_category_info(
                main_name, level=1
            )
            sub_id, sub_code, actual_sub_name = (
                self.get_category_info(sub_name, level=2, parent_voc_id=main_code)
                if main_code
                else (None, None, None)
            )
            detail_id, detail_code, actual_detail_name = (
                self.get_category_info(detail_name, level=3, parent_voc_id=sub_code)
                if sub_code
                else (None, None, None)
            )

            resolved_categories.append(
                {
                    "main_id": main_id,
                    "main_code": main_code,
                    "main_name": actual_main_name or main_name,
                    "sub_id": sub_id,
                    "sub_code": sub_code,
                    "sub_name": actual_sub_name or sub_name,
                    "detail_id": detail_id,
                    "detail_code": detail_code,
                    "detail_name": actual_detail_name or detail_name,
                    "detail_reason": detail_reason,
                }
            )

        # Sort resolved categories: push "Í∏∞ÌÉÄ-Í∏∞ÌÉÄ-Í∏∞ÌÉÄ" to the end
        def is_all_Í∏∞ÌÉÄ_resolved(resolved):
            """Check if all resolved category names are 'Í∏∞ÌÉÄ'"""
            if not resolved or (
                resolved["main_code"] is None
                and resolved["sub_code"] is None
                and resolved["detail_code"] is None
            ):
                return True  # Empty categories go to end
            return (
                resolved.get("main_name") == "Í∏∞ÌÉÄ"
                and resolved.get("sub_name") == "Í∏∞ÌÉÄ"
                and resolved.get("detail_name") == "Í∏∞ÌÉÄ"
            )

        def is_empty_resolved(resolved):
            """Check if category is empty (all codes are None)"""
            if not resolved:
                return True
            return (
                resolved["main_code"] is None
                and resolved["sub_code"] is None
                and resolved["detail_code"] is None
            )

        # Separate into: proper categories, Í∏∞ÌÉÄ-Í∏∞ÌÉÄ-Í∏∞ÌÉÄ, and empty categories
        proper_categories = [
            c
            for c in resolved_categories
            if not is_all_Í∏∞ÌÉÄ_resolved(c) and not is_empty_resolved(c)
        ]
        Í∏∞ÌÉÄ_categories = [
            c
            for c in resolved_categories
            if is_all_Í∏∞ÌÉÄ_resolved(c) and not is_empty_resolved(c)
        ]
        empty_categories = [c for c in resolved_categories if is_empty_resolved(c)]

        # Reorder: proper categories first, then Í∏∞ÌÉÄ-Í∏∞ÌÉÄ-Í∏∞ÌÉÄ, then empty
        resolved_categories = proper_categories + Í∏∞ÌÉÄ_categories + empty_categories

        # Track seen category combinations for deduplication
        seen_category_combinations = set()

        for i, resolved in enumerate(resolved_categories):
            level_num = i + 1

            # Create a unique key for this category combination
            category_key = (
                resolved["main_code"],
                resolved["sub_code"],
                resolved["detail_code"],
            )

            # Check for duplicate: skip if already seen or if all codes are None
            is_duplicate = category_key in seen_category_combinations
            is_empty = category_key == (None, None, None)

            if is_duplicate and not is_empty:
                print(
                    f"üîÑ Skipping duplicate category combination at level {level_num}: {category_key}"
                )
                sample_output.update(
                    {
                        f"main_category_{level_num}_name": None,
                        f"main_category_{level_num}_id": None,
                        f"main_category_{level_num}_code": None,
                        f"sub_category_{level_num}_name": None,
                        f"sub_category_{level_num}_id": None,
                        f"sub_category_{level_num}_code": None,
                        f"detail_category_{level_num}_name": None,
                        f"detail_category_{level_num}_id": None,
                        f"detail_category_{level_num}_code": None,
                        f"detail_category_{level_num}_reason": None,
                    }
                )
            else:
                # Add to seen set if not empty
                if not is_empty:
                    seen_category_combinations.add(category_key)

                sample_output.update(
                    {
                        f"main_category_{level_num}_name": resolved["main_name"],
                        f"main_category_{level_num}_id": resolved["main_id"],
                        f"main_category_{level_num}_code": resolved["main_code"],
                        f"sub_category_{level_num}_name": resolved["sub_name"],
                        f"sub_category_{level_num}_id": resolved["sub_id"],
                        f"sub_category_{level_num}_code": resolved["sub_code"],
                        f"detail_category_{level_num}_name": resolved["detail_name"],
                        f"detail_category_{level_num}_id": resolved["detail_id"],
                        f"detail_category_{level_num}_code": resolved["detail_code"],
                        f"detail_category_{level_num}_reason": resolved[
                            "detail_reason"
                        ],
                    }
                )

        print(sample_output)
        return sample_output

    def analyze_message_in_batch(self, samples: pd.DataFrame):
        if self.categories is None or self.voc_categories_df is None:
            try:
                self.build_input_categories()
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to build input categories: {e}")

        payload = {
            "input_texts": [
                self.input_text_cleansing(sample.content)
                for _, sample in samples.iterrows()
            ],
            "input_categories": self.categories,
            "max_new_tokens": 512,
            "temperature": 0.1,
            "top_p": 0.9,
        }
        response = requests.post(f"{self.api_url}/batch", json=payload)
        print(f"Batch API Response Status: {response.status_code}")
        if response.status_code == 200:
            parsed_responses = [item.get("parsed_response") for item in response.json()]
            confidence_scores = [
                item.get("confidence_score", 0.0) for item in response.json()
            ]
        else:
            print(f"API Error: {response.text}")
            parsed_responses = [None] * len(samples)
            confidence_scores = [0.0] * len(samples)

        sample_outputs = []
        for i, (_, sample) in enumerate(samples.iterrows()):
            sllm_response = parsed_responses[i]
            confidence_score = confidence_scores[i]

            # Use shared postprocess logic
            sample_output = self._postprocess_response(
                sample, sllm_response, confidence_score
            )
            sample_outputs.append(sample_output)

        return sample_outputs

    def analyze_message(self, sample):
        """Analyze a single sample (pandas namedtuple or similar). Returns the sample_output dict.

        Expects sample to have attributes: rcno, ccod, msg_id, received_at, content
        """
        # Ensure categories and voc categories are built
        if self.categories is None or self.voc_categories_df is None:
            try:
                self.build_input_categories()
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to build input categories: {e}")

        payload = {
            "input_text": self.input_text_cleansing(sample.content),
            "input_categories": self.categories,
            "max_new_tokens": 512,
            "temperature": 0.1,
            "top_p": 0.9,
        }

        response = requests.post(f"{self.api_url}/predict", json=payload)
        print(f"Sample RCNO: {sample.rcno}")
        print(f"API Response Status: {response.status_code}")
        if response.status_code == 200:
            response_json = response.json()
            sllm_response = response_json.get("parsed_response")
            confidence_score = response_json.get("confidence_score", 0.0)
            print(f"SLLM Response: {sllm_response}")
        else:
            print(f"API Error: {response.text}")
            sllm_response = None
            confidence_score = 0.0

        # Use shared postprocess logic
        return self._postprocess_response(sample, sllm_response, confidence_score)
